---
title: "ManyBabies 1: Pilot Data Analysis"
author: "ManyBabies Analysis Group (inc. Mike Frank, Hugh Rabagliati, Melissa Kline)"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
    highlight: tango
    theme: spacelab
---

This document shows an analysis of MB1 pilot data for purposes of procedural and analytic decision-making. It includes analysis of demographics and looking times, as well as condition differences. 

Please forgive errors, omissions, lack of clearly labeled axes, etc. This document was produced quickly to provide an initial guide for decision-making. 

# Decision points

There are many decision-points that need to be discussed throughout. 

**Data formatting**: Data upload script plus templates are [here](https://docs.google.com/document/d/1SqUJIwvswVa2-8W_ijJBIO2NE9AR7FI1Q9DT15vWlZE/edit?usp=drive_web). They are designed to make merging demographic data easy. Please take a look.

**Exclusion criteria**: Are listed in [the manuscript](https://docs.google.com/document/d/1kv3zZ2ylKHlfu779Xw8DaKxUEBHAa56B-4sv-GRvuBU/edit):

Participants must be:
- monolingual (>90%)
- full term (37+ weeks)
- looking on at least one pair of test trials (a matched IDS/ADS pair)

Trials must be:
- longer than 2s (to allow viewing of the stimulus)


Decision points:

- Any other exclusions?
- Use of a meta-analytic model or a mixed-effects model?
- How to compute standardized effect size (see the relevant section)
- Any procedural modifications?

# Analytic Preliminaries

```{r Preliminaries, results = "hide", message = FALSE}
options(dplyr.width = Inf)
knitr::opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
library(tidyverse)
library(knitr)
library(langcog)
library(metafor)
library(lme4)

theme_set(theme_bw())
```

**NOTE**: Loading data computed in `munge_pilot_data.Rmd`.

```{r}
d <- suppressWarnings(read_csv("processed_data/pilot_data.csv", col_types = cols()))
```

How many participants in each group?

```{r}
d %>%
  group_by(lab, age_days, subid) %>%
  distinct %>%
  group_by(lab) %>%
  summarise(n = n(), 
            age_months = mean(age_days)/30.3) %>%
  kable(digits = 1)
```

## Exclusions

This is an important decision-point. At least we want to exclude:

- Children with no looking time
- Trials with no looking time 
- Trials with looking time below 2s - QUESTION: do we want to exclude for less than 2s _trial time_ or _looking time_? 

```{r}
lt_totals <- d %>%
  group_by(lab, subid) %>%
  summarise(total_looking_time = sum(looking_time, na.rm=TRUE))

d <- d %>%
  left_join(lt_totals) %>%
  filter(total_looking_time != 0, 
         !is.na(total_looking_time), 
         trial_type %in% c("IDS","ADS")) %>%
  mutate(looking_time = ifelse(looking_time < 2, NA, looking_time))
  

```

## Demographics

What's our participant age distribution?

```{r}
subs <- d %>%
  group_by(lab, subid, age_days) %>%
  distinct

qplot(age_days/30.3, binwidth = 1, fill = lab, data=subs) + 
  xlab("Age (months)")
```

## Looking time dynamics

First, the overall distribution of looking times. Note, Brookes data goes to 30s due to an error in coding the experiment.

```{r}
qplot(looking_time, fill = lab, facets = ~ lab, binwidth = 1, data = d) + 
  xlim(0,30) + 
  xlab("Looking time (s)")
```

Stanford has a large number of 2s looking times because that's the lookaway from the tracker. So when a child isn't looking at all, they get a 2s. *How should we deal with this?*

```{r}
qplot(age_days, looking_time, col = lab, data = d) + 
  geom_smooth(aes(group=1), method = "lm")
```

## Child outcomes

Next, are children making it through the experiment? Once exclusions are computed, we see that many kids are habituating and are not making it throughout the study. 

```{r}
d %>%
  group_by(lab, subid) %>%
  summarize(max_trial = max(trial_num[!is.na(looking_time)])) %>%
  summarise(prop_finishing = mean(max_trial == 8)) %>%
  kable(digits = 2)
```

Now, histogram of looking time by trial number. Looks like looking times are declining across trials, but not tremendously.

```{r}
ms <- d %>%
  group_by(trial_num) %>%
  summarise(looking_time = mean(looking_time, na.rm=TRUE))

ggplot(d, aes(x = looking_time)) + 
  geom_histogram(binwidth = 1, aes(fill = lab)) + 
  geom_vline(data = ms, aes(xintercept = looking_time), lty = 2) + 
  facet_wrap(~trial_num) + 
  xlim(c(0,30))
```


Plot means. Note that this graph has survivorship bias -- namely, those observations later in the graph represent kids that had more trials. 

```{r}
ms <- d %>%
  group_by(lab, trial_num) %>%
  multi_boot_standard(col = "looking_time", na.rm=TRUE)

ggplot(ms, aes(x = trial_num, y = mean, col = lab)) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
                 position = position_dodge(width = .1)) 
```


# IDS-ADS condition differences

We will be pursuing a within-subjects analysis approach, so almost all of our analysis will happen over pairs of trials. The downside of this approach is that if you are missing data from one of the trials in a pair, you are missing the difference score for that pair.  

```{r}
diffs <-  d %>%
  select(-total_looking_time) %>%
  spread(trial_type, looking_time) %>%
  mutate(diff = IDS - ADS) 
```

# Distribution

What's the distributional form of these difference score data? 

```{r}
qplot(diff, binwidth = 1, 
      data = filter(diffs, !is.na(diff))) + 
  geom_vline(xintercept = mean(diffs$diff), col = "red", lty = 2) 
```
Interestingly, it's not skewed, but it does have very overdispersed shape with a big strong peak in the middle and then long tails). 

Note spike near 0 is not due to low-looking time Stanford kids because LTs < 2s have been removed. This is legitimate data. 

But: `r round(mean(is.na(diffs$diff)), digits = 2)` of LTs have missing data. That's problematic.

```{r}
diffs %>%
  group_by(lab) %>%
  summarise(missing = mean(is.na(diff))) %>%
              kable(digits = 2)
```

Stanford data are almost all missing! In hindsight, kids were old, the eye-tracker didn't pick them up, looks were very short, etc. 

# IDS-ADS difference patterns

How does the IDS-ADS difference change with trials?

```{r}
ms_diff <- diffs %>%
  
  group_by(lab, trial_num) %>%
  multi_boot_standard(col = "diff", na.rm=TRUE)

ggplot(ms_diff, aes(x = trial_num, y = mean)) +
         geom_smooth(se = FALSE, span = 2) + 
  facet_wrap(~lab) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                 position = position_dodge(width= .1)) +
  ylab("IDS preference (s)") + 
  geom_hline(yintercept = 0, lty = 2)
```

Brookes and Stanford both have a number of trials wehre there is essentially no data to estimate. In contrast, if anything, UBC shows hints of a preference by the end of the study, with longer looks all throughout (and much younger babies). 

How does difference change with age? (by subject) 

```{r}
mss_diffs <- diffs %>%
  group_by(lab, subid) %>%
  summarise(age_days = mean(age_days), 
            diff = mean(diff, na.rm=TRUE))

qplot(age_days, diff, col = lab, group = 1, data = mss_diffs) + 
  geom_smooth(method = "lm") + 
  geom_hline(yintercept = 0, lty = 2) + 
  ylab("IDS preference (s)") 
```
         

# Meta-analytic approach

Following suggestions by Alex Cristia, who argued that this is a more straightforward approach and also has been followed in ManyLabs and the RRRs previously. In addition, it doesn't require knowing the full form of the required mixed-effects model (e.g., trial order effects, age x trial order interactions, quadratic habituation, etc.).

Compute effect size for each lab. This analysis follows the recommendation in [Jake Westfall's blogpost](http://jakewestfall.org/blog/index.php/2016/03/25/five-different-cohens-d-statistics-for-within-subject-designs/), which says that "classic" Cohen's $d$ is the difference of the means divided by the pooled standard deviation across conditions. 

```{r}
source("ma_helper.R")

ages <- d %>%
  group_by(lab, subid) %>%
  summarise(age_days = mean(age_days)) %>%
  summarise(age_days = mean(age_days))
  
ds <- diffs %>%
  group_by(lab, method) %>%
  summarise(d = mean(IDS - ADS, na.rm=TRUE) / 
              sqrt(mean(diag(var(cbind(IDS, ADS), na.rm=TRUE)))), 
            n = length(unique(subid)), 
            d_var = d_var_calc(n,d)) %>%
  left_join(ages)
```

```{r}
ggplot(ds, aes(x = age_days, y = d)) + 
  geom_point(aes(size = n, col = method)) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
  geom_smooth(method = "lm") + 
  # scale_colour_solarized(name = "", labels = labels, guide = guide) +
  scale_size_continuous(guide = FALSE) +
  xlab("\nMean Subject Age (Months)") +
  ylab("Effect Size\n")
```

Model with no age moderation.

```{r}
mod <- metafor::rma(d ~ 1, vi = d_var, slab = lab, data = ds, method = "REML") 
summary(mod)
```

```{r}
f <- fitted(mod)
p <- predict(mod)

alpha <- .05

forest_data <- data.frame(effects = as.numeric(mod$yi.f),
                          variances = mod$vi.f) %>%
  mutate(effects.cil = effects -
           qnorm(alpha / 2, lower.tail = FALSE) * sqrt(variances),
         effects.cih = effects +
           qnorm(alpha / 2, lower.tail = FALSE) * sqrt(variances),
         estimate = as.numeric(f),
         lab = names(f),
         estimate.cil = p$ci.lb,
         estimate.cih = p$ci.ub,
         inverse_vars = 1/variances,
         identity = 1) %>%
  left_join(ds) 

qplot(lab, effects, ymin = effects.cil, ymax = effects.cih,
        geom = "linerange",
        data = forest_data) +
    geom_point(aes(y = effects, size = inverse_vars, col = method)) +
    geom_pointrange(aes(x = lab, y = estimate,
                               ymin = estimate.cil, ymax = estimate.cih),
                    pch = 17) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
    coord_flip() +
    scale_size_continuous(guide = FALSE) +
    scale_colour_solarized() +
    xlab("") +
    ylab("Effect Size")

```

Model with age moderation.

```{r}
mod <- metafor::rma(d ~ age_days, vi = d_var, slab = lab, data = ds, method = "REML") 
summary(mod)
```

Model with age x language interaction. (Not shown because right now we only have English data. 

```{r, eval = FALSE}
ds$language <- "english"
mod <- metafor::rma(d ~ age_days * language, vi = d_var, slab = lab, data = ds, method = "REML") 
summary(mod)
```


Model with method moderation.

```{r}
mod <- metafor::rma(d ~ method, vi = d_var, slab = lab, data = ds, method = "REML") 
summary(mod)
```

# Mixed effects approach

This approach is based on modeling *all* difference scores.

**Decision point**: will we log transform? Take differences? Take log differences or difference of logs?

**These models all need item effects but we don't have item data in the dataframe yet.**

## Simple models

This is the most basic model:

```{r}
d_lmer <- diffs %>% 
  filter(!is.na(diff)) %>%
  mutate(age_centered = scale(age_days))

summary(lmer(diff ~ 1 + 
               (1 | lab) + 
               (1 | subid) , data = d_lmer))
```

and age effects. 

```{r}
summary(lmer(diff ~ age_centered + 
               (age_centered | lab) + 
               (1 | subid) , data = d_lmer))
```

Neither of these models take into account the shape of the habituation curve or how preference might manifest across it. 

## More complete model

Add trial number X age interaction.

```{r}
summary(lmer(diff ~ age_centered * trial_num + 
               (age_centered * trial_num | lab) + 
               (trial_num | subid) , data = d_lmer))
```

and the quadratic version of that analysis. 

```{r}
summary(lmer(diff ~ age_centered * trial_num + 
               (age_centered * trial_num | lab) + 
               (trial_num | subid) , data = d_lmer))
```


<!-- ## IDS Preference across age -->

<!-- #### Linear effect of age -->

<!-- Planned regression: 1 + CentredAge + (1 + Centred Age | lab)   -->


<!-- ```{r} -->

<!-- summary(lmer(log_lt_diff ~ 1 + AgeC + (1+ AgeC |lab), data = d_lmer0)) -->
<!-- ``` -->

<!-- #### Quadratic effect of age -->

<!-- Planned regression: 1 + (CentredAge + CentredAge^2)  + (1 + (Centred Age + CentredAge ^2) "* Trial Type| lab)   -->

<!-- I removed the by lab interaction to aid convergence -->

<!-- ```{r} -->
<!-- summary(lmer(log_lt_diff ~ 1 + poly(AgeC,2) + (1+ poly(AgeC,2) |lab), data = d_lmer0)) -->
<!-- ``` -->

<!-- #### Secondary hypothesis tests, e.g trial order and age.  -->
<!-- We will fit a linear mixed effects model predicting all individual observations, with the structure:   -->
<!-- log(looking.time) ~ trial.num * stimulus * age + (trial.num * stimulus | subid) + (trial.num * stimulus * age | lab)   -->
<!-- NB. This is taken from the RRR. Does stimulus here refer to condition or item? I have taken it to refer to condition.   -->
<!-- Interactions removed to aid convergence. -->

<!-- *mk: why not mixed effects structures in the other models as well?  I think we have 3 sources of random variance we want to model here: random subids nested within random labs, and random stimuli (indivudal items). * -->

<!-- ```{r} -->
<!-- d_lmer3 <- d %>%  -->
<!--   filter(trial_type != "Train",  -->
<!--          looking_time != 0, !is.na(looking_time)) %>% -->
<!--   mutate(log_lt = log(looking_time), AgeC = (age_days - mean(age_days))/sd(age_days),trial_numC = (trial_num - mean(trial_num))/sd(trial_num))  -->
<!-- summary(lmer(log_lt ~ 1 + AgeC * trial_type * trial_numC + (1+ trial_type + trial_numC|subid)+ (1+ AgeC + trial_type + trial_numC|lab) , data = d_lmer3)) -->
<!-- ``` -->



<!-- ## Christina's Planned Moderator analyses -->
<!-- These analyses were planned to be by preference score, rather than by interactions. **Iff we are able to include item names in the data files, I would suggest going by interactions instead so that we can account for item effects with regard to, e.g., missing data.**    *cant we measure item effeects on preference scores too, since the items are paired across condition? -mk* -->

<!-- #### Preference by method -->
<!-- Note that this analysis is perfectly confounded with lab at the moment, so it will not converge.    -->
<!-- ids.pref ~ method + (1 | lab) -->

<!-- ```{r} -->
<!-- d_moder1 <- d %>%  -->
<!--   filter(trial_type != "Train",  -->
<!--          looking_time != 0, !is.na(looking_time)) %>% -->
<!--   mutate(log_lt = log(looking_time),AgeC = (age_days - mean(age_days))/sd(age_days)) %>%  -->
<!--   group_by(subid,trial_type,method,lab,AgeC) %>%  -->
<!--   summarise(log_lt = mean(log_lt)) %>% -->
<!--   group_by(subid,method,lab,AgeC) %>%  -->
<!--   filter(n() == 2) %>% -->
<!--   summarize(log_lt_diff = log_lt[trial_type == "IDS"] - log_lt[trial_type == "ADS"]) -->

<!-- summary(lmer(log_lt_diff ~ method + (1|lab), data = d_moder1)) -->
<!-- ``` -->

<!-- #### Preference by method interacting with age -->
<!-- Note that this analysis is perfectly confounded with lab at the moment, so it will not converge.    -->
<!-- ids.pref ~ method * age + (1 +age | lab) -->

<!-- ```{r} -->

<!-- summary(lmer(log_lt_diff ~ method*AgeC + (1+AgeC|lab), data = d_moder1)) -->
<!-- ``` -->
<!-- ## Melissa's planned 'analysis flexiblity' analysis -->
<!-- The general idea/approach is to specify a generative model of data inclusion (e.g. select subids to include, trials to include, lt windows to analyze), data analysis (preference vs. difference-from-chance, t test, etc., more things I haven't thought of), and optional stopping, and then to do some simulations of fitting this model to each lab maximizing effect size (essentially simulating 'worst case', intentional p-hacking) as well as some more limited models that e.g. select windows of analysis after data collection, stop early, drop participants, etc in a less nefarious way. -->

<!-- If we have interesting moderator findings, it would also be nice to see e.g. what kinds of sensible measurements you can make on standardized large datasets that you might **not** be able to do on heterogeneous data -->

